{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport sklearn\nfrom lime import explanation\nfrom lime import lime_base\nimport math\nimport logging\n\nclass TSDomainMapper(explanation.DomainMapper):\n    def __init__(self, signal_names, num_slices, is_multivariate):\n        \"\"\"Init function.\n        Args:\n            signal_names: list of strings, names of signals\n        \"\"\"\n        self.num_slices = num_slices\n        self.signal_names = signal_names\n        self.is_multivariate = is_multivariate\n        \n    def map_exp_ids(self, exp, **kwargs):\n        # in case of univariate, don't change feature ids\n        if not self.is_multivariate:\n            return exp\n        \n        names = []\n        for _id, weight in exp:\n            # from feature idx, extract both the pair number of slice\n            # and the signal perturbed\n            nsignal = int(_id / self.num_slices)\n            nslice = _id % self.num_slices\n            signalname = self.signal_names[nsignal]\n            featurename = \"%d - %s\" % (nslice, signalname)\n            names.append((featurename, weight))\n        return names\n\nclass LimeTimeSeriesExplainer(object):\n    \"\"\"Explains time series classifiers.\"\"\"\n\n    def __init__(self,\n                 kernel_width=25,\n                 verbose=False,\n                 class_names=None,\n                 feature_selection='auto',\n                 signal_names=[\"not specified\"]\n                 ):\n        \"\"\"Init function.\n        Args:\n            kernel_width: kernel width for the exponential kernel\n            verbose: if true, print local prediction values from linear model\n            class_names: list of class names, ordered according to whatever the\n            classifier is using. If not present, class names will be '0',\n                '1', ...\n            feature_selection: feature selection method. can be\n                'forward_selection', 'lasso_path', 'none' or 'auto'.\n            signal_names: list of strings, names of signals\n        \"\"\"\n\n        # exponential kernel\n        def kernel(d): return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n\n        self.base = lime_base.LimeBase(kernel, verbose)\n        self.class_names = class_names\n        self.feature_selection = feature_selection\n        self.signal_names = signal_names\n\n    def explain_instance(self,\n                         timeseries_instance,\n                         classifier_fn,\n                         num_slices,\n                         labels=(1,),\n                         top_labels=None,\n                         num_features=10,\n                         num_samples=5000,\n                         model_regressor=None,\n                         replacement_method='mean'):\n        \"\"\"Generates explanations for a prediction.\n\n        First, we generate neighborhood data by randomly hiding features from\n        the instance (see __data_labels_distance_mapping). We then learn\n        locally weighted linear models on this neighborhood data to explain\n        each of the classes in an interpretable way (see lime_base.py).\n        As distance function DTW metric is used.\n\n        Args:\n            time_series_instance: time series to be explained.\n            classifier_fn: classifier prediction probability function,\n                which takes a list of d arrays with time series values\n                and outputs a (d, k) numpy array with prediction\n                probabilities, where k is the number of classes.\n                For ScikitClassifiers , this is classifier.predict_proba.\n            num_slices: Defines into how many slices the time series will\n                be split up\n            labels: iterable with labels to be explained.\n            top_labels: if not None, ignore labels and produce explanations for\n            the K labels with highest prediction probabilities, where K is\n            this parameter.\n            num_features: maximum number of features present in explanation\n            num_samples: size of the neighborhood to learn the linear model\n            distance_metric: the distance metric to use for sample weighting,\n                defaults to cosine similarity\n            model_regressor: sklearn regressor to use in explanation. Defaults\n                to Ridge regression in LimeBase. Must have\n                model_regressor.coef_ and 'sample_weight' as a parameter to\n                model_regressor.fit()\n        Returns:\n            An Explanation object (see explanation.py) with the corresponding\n            explanations.\n       \"\"\"\n\n        permutations, predictions, distances = self.__data_labels_distances(\n            timeseries_instance, classifier_fn,\n            num_samples, num_slices, replacement_method)\n\n        is_multivariate = len(timeseries_instance.shape) > 1\n        \n        if self.class_names is None:\n            self.class_names = [str(x) for x in range(predictions[0].shape[0])]\n\n        domain_mapper = TSDomainMapper(self.signal_names, num_slices, is_multivariate)\n        ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n                                          class_names=self.class_names)\n        ret_exp.predict_proba = predictions[0]\n\n        if top_labels:\n            labels = np.argsort(predictions[0])[-top_labels:]\n            ret_exp.top_labels = list(predictions)\n            ret_exp.top_labels.reverse()\n        for label in labels:\n            (ret_exp.intercept[int(label)],\n             ret_exp.local_exp[int(label)],\n             ret_exp.score,\n             ret_exp.local_pred) = self.base.explain_instance_with_data(\n                permutations, predictions,\n                distances, label,\n                num_features,\n                model_regressor=model_regressor,\n                feature_selection=self.feature_selection)\n        return ret_exp\n\n    def __data_labels_distances(cls,\n                                timeseries,\n                                classifier_fn,\n                                num_samples,\n                                num_slices,\n                                replacement_method='mean'):\n        \"\"\"Generates a neighborhood around a prediction.\n\n        Generates neighborhood data by randomly removing slices from the\n        time series and replacing them with other data points (specified by\n        replacement_method: mean over slice range, mean of entire series or\n        random noise). Then predicts with the classifier.\n\n        Args:\n            timeseries: Time Series to be explained.\n                it can be a flat array (univariate)\n                or (num_signals, num_points) (multivariate)\n            classifier_fn: classifier prediction probability function, which\n                takes a time series and outputs prediction probabilities. For\n                ScikitClassifier, this is classifier.predict_proba.\n            num_samples: size of the neighborhood to learn the linear\n                model (perturbation + original time series)\n            num_slices: how many slices the time series will be split into\n                for discretization.\n            replacement_method:  Defines how individual slice will be\n                deactivated (can be 'mean', 'total_mean', 'noise')\n        Returns:\n            A tuple (data, labels, distances), where:\n                data: dense num_samples * K binary matrix, where K is the\n                    number of slices in the time series. The first row is the\n                    original instance, and thus a row of ones.\n                labels: num_samples * L matrix, where L is the number of target\n                    labels\n                distances: distance between the original instance and\n                    each perturbed instance\n        \"\"\"\n\n        def distance_fn(x):\n            return sklearn.metrics.pairwise.pairwise_distances(\n                x, x[0].reshape([1, -1]), metric='cosine').ravel() * 100\n\n        num_channels = 1\n        len_ts = len(timeseries)\n        if len(timeseries.shape) > 1:  # multivariate\n            num_channels, len_ts = timeseries.shape\n        \n        values_per_slice = math.ceil(len_ts / num_slices)\n        deact_per_sample = np.random.randint(1, num_slices + 1, num_samples - 1)\n        perturbation_matrix = np.ones((num_samples, num_channels, num_slices))\n        features_range = range(num_slices)\n        original_data = [timeseries.copy()]\n\n        for i, num_inactive in enumerate(deact_per_sample, start=1):\n            logging.info(\"sample %d, inactivating %d\", i, num_inactive)\n            # choose random slices indexes to deactivate\n            inactive_idxs = np.random.choice(features_range, num_inactive,\n                                             replace=False)\n            num_channels_to_perturb = np.random.randint(1, num_channels+1)\n\n            channels_to_perturb = np.random.choice(range(num_channels),\n                                                   num_channels_to_perturb,\n                                                   replace=False)\n            \n            logging.info(\"sample %d, perturbing signals %r\", i,\n                         channels_to_perturb)\n            \n            for chan in channels_to_perturb:\n                perturbation_matrix[i, chan, inactive_idxs] = 0\n                \n            tmp_series = timeseries.copy()\n\n            for idx in inactive_idxs:\n                start_idx = idx * values_per_slice\n                end_idx = start_idx + values_per_slice\n                end_idx = min(end_idx, len_ts)\n\n                if replacement_method == 'mean':\n                    # use mean of slice as inactive\n                    perturb_mean(tmp_series, start_idx, end_idx,\n                                 channels_to_perturb)\n                elif replacement_method == 'noise':\n                    # use random noise as inactive\n                    perturb_noise(tmp_series, start_idx, end_idx,\n                                  channels_to_perturb)\n                elif replacement_method == 'total_mean':\n                    # use total series mean as inactive\n                    perturb_total_mean(tmp_series, start_idx, end_idx,\n                                       channels_to_perturb)\n            original_data.append(tmp_series)\n\n        predictions = classifier_fn(np.array(original_data))\n        \n        # create a flat representation for features\n        perturbation_matrix = perturbation_matrix.reshape((num_samples, num_channels * num_slices))\n        distances = distance_fn(perturbation_matrix)\n\n        return perturbation_matrix, predictions, distances\n\ndef perturb_total_mean(m, start_idx, end_idx, channels):\n    # univariate\n    if len(m.shape) == 1:\n        m[start_idx:end_idx] = m.mean()\n        return\n    \n    for chan in channels:\n        m[chan][start_idx:end_idx] = m[chan].mean()\n\ndef perturb_mean(m, start_idx, end_idx, channels):\n    # univariate\n    if len(m.shape) == 1:\n        m[start_idx:end_idx] = np.mean(m[start_idx:end_idx])\n        return\n    \n    for chan in channels:\n        m[chan][start_idx:end_idx] = np.mean(m[chan][start_idx:end_idx])\n        \ndef perturb_noise(m, start_idx, end_idx, channels):\n    # univariate\n    if len(m.shape) == 1:\n        m[start_idx:end_idx] = np.random.uniform(m.min(), m.max(),\n                                                 end_idx - start_idx)\n        return\n\n    for chan in channels:\n        m[chan][start_idx:end_idx] = np.random.uniform(m[chan].min(),\n                                                       m[chan].max(),\n                                                       end_idx - start_idx)","metadata":{"_uuid":"d121ceec-1a12-4107-9923-72a469b6cfcd","_cell_guid":"967ca6f0-8472-4ec1-b728-cbfc50aae7d2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}